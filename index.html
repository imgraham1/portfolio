<!DOCTYPE html>
<html lang="en" dir="ltr">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="css/html5reset.css">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <title>Iain Graham</title>
    <link rel="icon" href="images/rounded.png">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NJS3QJ1GD4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NJS3QJ1GD4');
    </script>




  </head>



  <body>

    <div class='sticky'>
    <header>
      <div class='my_logo'>
        <a href='#' id='pls'><img class='logo' src='images/name.png' alt='name logo' height=75px width=400px></a>
      </div>
      <nav id='navigaition'>
        <ul class='nav_links'>
          <li><a href='#project_section'>Projects</a></li>
          <li><a href='#skills'>Skills</a></li>
          <li><a href='#course_section'>Coursework</a></li>
          <li><a href='#about_me'>About</a></li>
          <li><a href='#contact_me'>Contact</a></li>
        </ul>
      </nav>
<!--       <a href='pdfs/resume.pdf' class='cta'><button>Resume</button></a> -->
	    
	<a href='pdfs/Resume.pdf' class="cta"><button>Resume</button></a>
    </header>
  </div>



    <img src="images/pines1.jpg" alt="Grand tetons" style="width:100%;opacity:0.4;">
    <div class="text">
      Hi, my name is Iain.<br>
      <div class='small'>
        I'm an aspiring data professional
      </div>
  </div>







<div class='projects' id='project_section'>
<p id='project_title'>Projects<br><br></p>
<p>Recent Projects:</p>
<div class='proj_container'>

<!-- beginning one project entry -->
  <div class='project_entry'><h1>Detecting COVID-19 Cases Using Lung CT Scans</h1><br>
    <ul>
      <li>Machine Learning</li>
      <li>Neural Networks</li>
      <li>Deep Learning</li>
      <li>Data Augmentation</li>
    </ul>
    <br><br>
  <img src="images/lung.png" alt="lung" height=75px width=125px align='left' /><div>As the COVID-19 pandemic continues to effect people around the world, it is becoming increasingly important to develop novel methods for detecting whether or not an individual actually has the virus. This project explores a method of detecting positive cases of COVID-19 using deep learning and images of CT scans of lungs. We found a that it is possible to achieve an accuracy of roughly 90% after implementing pre-trained convolutional neural network models. The dataset we used contained 334 images of positive cases and 373 images of negative cases. To begin, we used data augmentation techniques to increase the size of the training dataset to a total of 2,121 images so that our networks would have better performance and better generalizability. We implemented a total of 4 different approaches to - a feed forward neural network using scikit learn and basic CNN using keras which were both trained from scratch, and we retrained the last layers of the resnet and VGG-16 pre-trained CNNs. As we expected, the neural network worst performance based off of the accuracy metric, followed by the basic CNN achieving accuracies of 62% and 74% respectively. The resnet model achieved an accuracy of 81% while the VGG-16 model achieved 91% accuracy. These results showed us that using images of CT scans of lungs could potentially provide a promising approach to classify whether or not an individual has COVID-19. Considering some of the concerns of validity/accuracy of current testing approaches, these findings become increasingly intriguing. Using the approaches we explored in this project have the potential to improve the ability to correctly diagnose cases of COVID-19 if we use them in combination with some of the current approaches.
  </div>
<br>
  <!-- <div class='project_footer'>
    See more on:<br><br>
    <ul>
      <li><a href='https://github.com/imgraham1'>GitHub</a></li>
      <li><a href='https://github.com/imgraham1'>Paper</a></li>
      <li><a href='https://github.com/imgraham1'>Poster</a></li>
      <li><a href='https://github.com/imgraham1'>Medium</a></li>
    </ul>
  </div> -->
  </div>
<!-- ending one project entry -->


<!-- beginning one project entry -->
  <div class='project_entry'><h1>Vertical Search Engine from Scratch</h1><br>
    <ul>
      <li>Information Retrieval</li>
      <li>NLP</li>
      <li>Flask</li>
    </ul>
    <br><br>
  <img src="images/shuttle.jpg" alt="shuttle" height=100px width=175px align='left' /><div>Vertical search engines help people access domain-specific information quickly. In a lot of cases, they are better resources for people to use compared to general-purpose search engines if they are in search of specific information that is related to a unique field. This purpose of this project is to build a vertical search engine from scratch surrounding space exploration and explore various term weighting ranking functions to bring the benefits of vertical search to an increasingly popular domain. I scraped data from a variety of different sources and ended up with a collection of over 2,000 on topics surrounding the shuttle and Apollo space flight missions. The documents were cleaned by lower casing and removing stop words, punctuation, and special characters. Additionally, documents that were shorter than 30 tokens (words) were dropped from the corpus, and document length was truncated at 2,000 tokens. To evaluate the performance of each ranking function, I created a set of 20 test queries and annotated the relevant “responses” that should be returned given the test query. Results were then evaluated based on recall and precision. I tested 7 different term-weighting ranking algorithms including TF-IDF, term frequency, BM-25, BM-25L, BM-25+, axiomatic approach, and divergence from randomness. The logic behind these algorithms is to rank documents based off of weighting terms that occur in a document, given the terms in the query. The best performing function in terms of speed and performance was the axiomatic approach, which is what I chose to power the final product. The engine is deployed using a Flask framework and hosted on PythonAnywhere.
  </div>
<br>
  <div class='project_footer'>
    See more on:<br><br>
    <ul>
      <li><a href='http://imgraham1.pythonanywhere.com/'>Finished product</a></li>
      <li><a href='https://imgraham1996.medium.com/building-deploying-a-vertical-search-engine-from-scratch-a48bde9447aa'>Medium</a></li>
      <!-- <li><a href='https://github.com/imgraham1'>Poster</a></li>
      <li><a href='https://github.com/imgraham1'>Medium</a></li> -->
    </ul>
  </div>
  </div>
<!-- ending one project entry -->



<!-- beginning one project entry -->
  <div class='project_entry'><h1>Echo Chambers in Twitter Political Networks</h1><br>
    <ul>
      <li>Networks</li>
      <li>Data Manipulation</li>
      <li>Time Series</li>
    </ul>
    <br><br>
  <img src="images/network.png" alt="network" height=150px width=250px align='left' /><div>Echo chambers occur in spaces when people of similar beliefs only interact with individuals that possess those same beliefs and are commonly found among topics that are highly polarizing. Examining political conversations on Twitter using network science can allow us to see the presence of echo chambers, and more interestingly how they evolve over time. We create and examine networks using Twitter data from March-November 2020 to see how characteristics of echo chambers change over time during the U.S.presidential election. To examine these networks, we first had to label ideological preferences of the Twitter users we had data for by using keywords and hashtags that were pre-determined as being either liberal or conservative. Once this step was completed, we constructed 9 different networks - 1 representing each month from Mar-Nov - where nodes represented users and un-directed links represented mentions between users (I.e. there is a link between a and b if a mentions b, or if b mentions a). We examined centrality measures of betweenness, degree, and XXX in each of these networks as a way to observe how their characteristics and presence of echo chambers evolve over time. We initially hypothesized that echo chambers would become more prevalent as the election neared, however we found that the concentration of users with the same political beliefs decreased across time, that is there was less likely to be echo chambers present as the election approached. These findings were interesting because they give evidence that people are less likely to find themselves in an echo chamber and more likely to be having conversation with people of opposing beliefs during an important point in politics when these interactions are undoubtedly important to have.
  </div>
<br>
  <!-- <div class='project_footer'>
    See more on:<br><br>
    <ul>
      <li><a href='https://github.com/imgraham1'>GitHub</a></li>
      <li><a href='https://github.com/imgraham1'>Paper</a></li>
      <li><a href='https://github.com/imgraham1'>Poster</a></li>
      <li><a href='https://github.com/imgraham1'>Medium</a></li>
    </ul>
  </div> -->
  </div>
<!-- ending one project entry -->

<!-- beginning one project entry -->
  <div class='project_entry'><h1>Word2Vec from Scratch</h1><br>
    <ul>
      <li>NLP</li>
      <li>Machine Learning</li>
    </ul>
    <br><br>
  <img src="images/word2vec.png" alt="nn diagram" height=100px width=200px align='left' /><div>Natural language processing allows us to apply data science techniques to human language. Unfortunately, many of the popular methods in areas like machine learning, among others, are not able to directly deal with raw text as input. Converting tokens into word vectors allows us to represent human language as in a way that lets us leverage more data science techniques. The word2vec algorithm is likely the most effective way to make this conversion because it allows words to retain their semantic meaning. In this project I implement the state of the art word2vec algorithm from scratch using python and a dataset of Amazon reviews. To begin I removed stopwords, punctuation, and special characters as well as replaced the rare words with an <unk> token and dropped the overly common words. These filter decisions helps the algorithm better capture the meaning of words by not learning from the noise of frequent or in-frequent occurrences. This algorithm learns representations of words by looking at the tokens before and after the target word so that the vectors capture their meaning. I built this algorithm using gradient descent and negative sampling, as well as included an adjustable window so the window before and after the target word can be a tunable parameter to ensure you learn the best representation of each word. Once the model ran, I was able to use the created vectors to see the most similar words to each other, complete analogies, add and subtract words, and use them in different machine learning/data science tasks. To evaluate the success of my implementation I computed the top 10 most similar words to each word in the vocabulary using cosine similarity to ensure that words that are similar (I.e. big and large) were amongst the most similar returned words.
  </div>
<br>
  <!-- <div class='project_footer'>
    See more on:<br><br>
    <ul>
      <li><a href='https://github.com/imgraham1'>GitHub</a></li>
      <li><a href='https://github.com/imgraham1'>Paper</a></li>
      <li><a href='https://github.com/imgraham1'>Poster</a></li>
      <li><a href='https://github.com/imgraham1'>Medium</a></li>
    </ul>
  </div> -->
  </div>
<!-- ending one project entry -->




</div>
</div>


<button type="button" class="collapsible"></button>
<div class="content">
  <p>
    <div class='proj_container'>




    <!-- beginning one project entry -->
      <div class='project_entry'><h1>A New Approach to Evaluating Summarization Methods</h1><br>
        <ul>
          <li>NLP</li>
          <li>Machine Learning</li>
          <li>Summarization</li>
        </ul>
        <br><br>
      <img src="images/pointer.png" alt="pointer" height=100px width=150px align='left' /><div>Summarization is meaningful because it simplifies long, complex pieces of text into shorter, more digestible documents. There are three main approaches to summarization - extractive (i.e. copying the most useful text directly from the main document), abstractive (i.e. generating new text that covers the most important parts of the main document), and hybrid models(i.e. copying and generating text). All of these methods are typically evaluated based on the ROUGE metric. This evaluation method is effective at telling us how much information from the original text is captured in the summary, however in this project I argue that it should not be the only metric that is considered because it fails to address whether or not the summary becomes less complex. I implement variations of the three different summarization methods - the TextRank extractive method, sequence2sequence abstractive method, and the state of the art pointer-generator approach for the hybrid approach, and evaluate them based on the Improvement in readability in addition to the ROUGE metric. Evaluating based on readability gives us a better idea of how effective the summary is by offering a measure of how much more comprehendible the summaries produced are. The pointer-generator model performed the best by achieving the highest ROUGE scores, as well as a meaningful decrease in complexity because of it’s architecture and ability to pull out a lot of the important text (e.g. what leads to the high ROUGE scores) as well it’s ability to generate new, less complex text (e.g. what leads to the decrease in text complexity).
      </div>
    <br>
      <div class='project_footer'>
        See more on:<br><br>
        <ul>
          <li><a href='https://imgraham1996.medium.com/exploring-a-different-approach-for-evaluating-summarization-methods-c6cefe28039e'>Medium</a></li>
          <!-- <li><a href='https://github.com/imgraham1'>Paper</a></li>
          <li><a href='https://github.com/imgraham1'>Poster</a></li>
          <li><a href='https://github.com/imgraham1'>Medium</a></li> -->
        </ul>
      </div>
      </div>
    <!-- ending one project entry -->


    <!-- beginning one project entry -->
      <div class='project_entry'><h1>AirBnB in NYC and the Effect on the Rental Market</h1><br>
        <ul>
          <li>Data Visualization</li>
          <li>Data Manipulation</li>
          <li>Storytelling</li>
        </ul>
        <br><br>
      <img src="images/nyc.jpg" alt="nyc" height=150px width=250px align='left' /><div>[note: this project was completed before COVID-19] AirBnB has revolutionized travel and the tourism industry - especially in cities that draw a lot of tourists. Allowing tenants to rent out excess space they have allows them to generate income while also giving someone a place to stay - a seemingly win-win scenario that makes both parties better off, however there is potential adverse effects on the rental market that could be harming tenants more than they think. In this project, we examine the relationship between airBnBs and the rental market in New York City through a series of informative and interactive visualizations. We used data from 2008 to 2020 that was collected from airBnB and streeteasy. The series of visualizations lets the data tell an interesting story and gives users the opportunity to interact with the data themselves to better understand the impacts of airBnB in NYC.
      </div>
    <br>
      <div class='project_footer'>
        See more on:<br><br>
        <ul>
          <li><a href='https://imgraham1.github.io/Airbnb_data_viz_project/'>Finished Product</a></li>
          <li><a href='https://github.com/imgraham1/Airbnb_data_viz_project'>GitHub</a></li>
          <!-- <li><a href='https://github.com/imgraham1'>Poster</a></li>
          <li><a href='https://github.com/imgraham1'>Medium</a></li> -->
        </ul>
      </div>
      </div>
    <!-- ending one project entry -->


    <!-- beginning one project entry -->
      <div class='project_entry'><h1>Text Classification from Scratch</h1><br>
        <ul>
          <li>NLP</li>
          <li>Machine Learning</li>
          <li>Text Classification</li>
        </ul>
        <br><br>
      <img src="images/class.jpg" alt="class" height=150px width=250px align='left' /><div>Text classification is a widely used method by NLP practitioners to determine if a given piece of text belongs to a certain category and I’d frequently used in tasks like sentiment analysis and spam detection. In this project, I code the näive bayes and logistic regression algorithms from scratch using Python to classify tweets as offensive or not offensive. The näive bayes algorithm classifies tweets by calculating the probability of the tweet being offensive given the tweet, as well as the probability of the tweet being not offensive given the tweet. Using this model I was able to achieve an accuracy of 79% correct classification. The logistic regression implementation uses a sigmoid activation function and gradient decent to learn a function to best approximate the output class given an input tweet. Using the logistic regression approach allowed me to achieve an accuracy of 82% correct classification. Coding both of these algorithms from scratch gave me a deep understanding of both approaches and a better understanding of how to correctly turn algorithms into code.
      </div>
    <br>
      <!-- <div class='project_footer'>
        See more on:<br><br>
        <ul>
          <li><a href='https://github.com/imgraham1'>GitHub</a></li>
          <li><a href='https://github.com/imgraham1'>Paper</a></li>
          <li><a href='https://github.com/imgraham1'>Poster</a></li>
          <li><a href='https://github.com/imgraham1'>Medium</a></li>
        </ul>
      </div> -->
      </div>
    <!-- ending one project entry -->


    <!-- beginning one project entry -->
      <div class='project_entry'><h1>Web Scraping and Databases</h1><br>
        <ul>
          <li>Web Scraping</li>
          <li>SQL</li>
          <li>Relational Databases</li>
          <li>Flask</li>
        </ul>
        <br><br>
      <img src="images/database.png" alt="database" height=150px width=250px align='left' /><div>In college athletics, schools are commonly labeled as being either a “basketball” school or a “football” school based on which team usually performs better. This project uses a combination of web scraping, databases, Python, and Flask to create a resource for people that allows them to quickly access information about a school’s basketball and football team so that they can determine which sport they are better at. I began this project by crawling three different websites using the beautifulsoup4 package to collect data on 236 college’s basketball and football teams and crawled a total of webpages to collect all of the data. As I scraped the data, I populated a SQLite relational database that contains 5 tables.  I used user inputs to generate SQL queries that would extract the relevant information to the users search term. I used Flask to give users an intuitive interface to create their queries and have the results displayed to them in an easy to read way.
      </div>
    <br>
      <!-- <div class='project_footer'>
        See more on:<br><br>
        <ul>
          <li><a href='https://github.com/imgraham1'>GitHub</a></li>
          <li><a href='https://github.com/imgraham1'>Paper</a></li>
          <li><a href='https://github.com/imgraham1'>Poster</a></li>
          <li><a href='https://github.com/imgraham1'>Medium</a></li>
        </ul>
      </div> -->
      </div>
    <!-- ending one project entry -->


    <!-- beginning one project entry -->
      <div class='project_entry'><h1>Consulting Project</h1><br>
        <ul>
          <li>Consulting</li>
          <li>Technology</li>
          <li>Data</li>
        </ul>
        <br><br>
      <img src="images/consult.png" alt="consult" height=150px width=250px align='left' /><div>In this project, I was a part of a group that was assigned to a small business in Ann Arbor, MI to help the owner make better use of their data and technology to increase revenue. We conducted 4 interviews with past customers and 2 meetings with the business owner to gather information that would help us make a recommendation on how they could implement to earn more revenue. We were also able to work with the data collected from business transactions and website traffic that we considered when constructing our implementation. After analyzing the data fromo our interviews and meetings, we noticed that the business was struggling to get repeat customers. Because of this, we chose to propose a recommendation that focused on this shortcoming. We suggested that the business owner implements a strict follow up to each transaction as well as an email newsletter so that they could stay in contact and remain engaged with their customers. We helped the business owner implement these recommendations by suggesting different technologies that they could leverage to ensure that this solution was easy to maintain.
      </div>
    <br>
      <!-- <div class='project_footer'>
        See more on:<br><br>
        <ul>
          <li><a href='https://github.com/imgraham1'>GitHub</a></li>
          <li><a href='https://github.com/imgraham1'>Paper</a></li>
          <li><a href='https://github.com/imgraham1'>Poster</a></li>
          <li><a href='https://github.com/imgraham1'>Medium</a></li>
        </ul>
      </div> -->
      </div>
    <!-- ending one project entry -->





    <!-- beginning one project entry -->
      <div class='project_entry'><h1>Topic Modelling</h1><br>
        <ul>
          <li>NLP</li>
          <li>Machine Learning</li>
          <li>Topic Modelling</li>
        </ul>
        <br><br>
      <img src="images/topic.png" alt="topic" height=150px width=250px align='left' /><div>Topic modeling is a way to discover the main topics/ideas that are present in a long pice of text. In this project I implement 3 different approaches of topic modeling applied to a dataset made up of random Wikipedia articles. The first approach I use is latent dirichlet allocation using Gibbs-sampling. The next two implementations I used were from the Gensim and Mallet  libraries. After analyzing the topics resulting from each method, I came to the conclusion that the Mallet implementation was the best approach because it was the fastest and produced the most accurate topics based on my own judgement. Working with these three different approaches for topic modeling gave me a good amount of experience working with topic modeling that are easily transferable to different real world problems.
      </div>
    <br>
      <!-- <div class='project_footer'>
        See more on:<br><br>
        <ul>
          <li><a href='https://github.com/imgraham1'>GitHub</a></li>
          <li><a href='https://github.com/imgraham1'>Paper</a></li>
          <li><a href='https://github.com/imgraham1'>Poster</a></li>
          <li><a href='https://github.com/imgraham1'>Medium</a></li>
        </ul>
      </div> -->
      </div>
    <!-- ending one project entry -->

</div>









  </p>
</div>







<div class='projects' id='skills'>
<p id='project_title'>Skills<br><br></p>
<p>Some of the languages, tools, and methods I have experience with</p>

<div class='proj_container'>

  <div class='skill_section'>
    <div>Python</div>
    <div>R</div>
    <div>SQL</div>
    <div>Jupyter</div>
    <div>Julia</div>
    <div>Tableau</div>
    <div>HTML & CSS</div>
    <div>Machine Learning</div>
    <div>Linear Regression</div>
    <div>Logistic Regression</div>
    <div>SVM</div>
    <div>Decision Trees</div>
    <div>Random Forest</div>
    <div>Boosting & Bagging</div>
    <div>XGBoost</div>
    <div>Regularization</div>
    <div>NLP</div>
    <div>Data Visualization</div>
    <div>Data Analysis</div>
    <div>Information Retrieval</div>
    <div>Data Mining</div>
    <div>Hadoop</div>
    <div>Spark</div>
    <div>Clustering</div>
    <div>K-Means Clustering</div>
    <div>Data Mining</div>
    <div>Pandas</div>
    <div>NumPy</div>
    <div>Seaborn</div>
    <div>Scikit Learn</div>
    <div>TensorFlow</div>
    <div>Altair</div>
    <div>PyTorch</div>
    <div>TF-IDF</div>
    <div>Neural Networks</div>
    <div>Deep Learning</div>
    <div>Convolutional Neural Nets</div>
    <div>PageRank</div>
    <div>Apriori Algorithm</div>
    <div>Web Scraping</div>
    <div>Map-Reduce</div>
    <div>Document Ranking</div>
    <div>Regular Expressions</div>
  </div>


</div>
</div>


<!-- Python (Pandas, NumPy, Seaborn, Altair, Plotly, Scikit learn, TensorFlow, PyTorch, keras, NLTK), R, machine learning(unsupervised & supervised learning, regression, classification, clustering), natural language processing (NLP), information visualization, data analysis, information retrieval, data mining, SQL, Hadoop, Spark, HTML, CSS, Tableau  -->










<div class='projects' id='course_section'>
<p id='project_title'>Coursework<br><br></p>

<body>
	<nav class="accordion1 arrows">
		<input type="radio" name="accordion1" id="cb1" />
		<section class="box">
			<label class="box-title" for="cb1"><img src="images/umich.png" alt="umich" height=75px width=75px align='left' />SI630: Natural Language Processing: Algorithms and People</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"This course focuses on how to use machine learning techniques to understand, annotate, and generate the language we see in everyday situations. The techniques learned in this course can be applied to any kind of text and enable turning qualitative evaluation of text in a precise quantitative measurement. Students will learn the linguistics fundamentals of natural language processing (NLP), with specific topics of part of speech tagging, syntax and parsing, lexical semantics, topic models, and machine translation. Additional advanced topics will include sentiment analysis, crowdsourcing, and deep learning for NLP."</div>
		</section>

    <input type="radio" name="accordion1" id="cb2" />
		<section class="box">
			<label class="box-title" for="cb2"><img src="images/umich.png" alt="umich" height=75px width=75px align='left' />SI670: Applied Machine Learning</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"Students will learn how to correctly apply, interpret results, and iteratively refine and tune supervised and unsupervised machine learning models to solve a diverse set of problems on real-world datasets. Application is emphasized over theoretical content."</div>
		</section>

    <input type="radio" name="accordion1" id="cb3" />
		<section class="box">
			<label class="box-title" for="cb3"><img src="images/umich.png" alt="umich" height=75px width=75px align='left' />SI671: Data Mining: Methods and Applications</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"With the explosive growth of information generated from different sources, in a variety of formats, and with various qualities, information analysis has become challenging for researchers in many disciplines. Automatic, robust, and intelligent data mining techniques have become essential tools to handle heterogeneous, noisy, nontraditional, and large-scale data sets. This is a seminar course of advanced topics in data mining, the state-of-the-art methods to analyze different genres of information, and the applications to many real world problems. The course will highlight the practical applications of data mining instead of the theoretical foundations of machine learning and statistical computing. The course materials will focus on how the information in different real world problems can be represented as particular genres, or formats of data, and how the basic mining tasks of each genre of data can be accomplished using the state-of-the-art techniques. To this end, the course is suitable for those who are consumers of data mining techniques in their own disciplines, such as natural language processing, networks science, human computer interaction, economics, social computing, sociology, business intelligence, and biomedical informatics, etc."</div>
		</section>

    <input type="radio" name="accordion1" id="cb4" />
		<section class="box">
			<label class="box-title" for="cb4"><img src="images/umich.png" alt="umich" height=75px width=75px align='left' />SI650: Information Retrieval</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"Information is everywhere. We encounter it in our everyday lives in the form of E-mail, newspapers, television, the Web, and even in conversations with each other. Information is hidden in a variety of media: text, images, sounds, videos. While casual information consumers can simply enjoy its abundance and appreciate the existence of search engines that can help them find what they want, information professionals are responsible for building the underlying technology that search engines use. Building a search engine involves a lot more than indexing some documents -- information retrieval is the study of the interaction between users and large information environments. It covers concepts such as information need, documents and queries, indexing and searching, retrieval evaluation, multimedia and hypertext search, Web search, as well as bibliographical databases. In this course, students go over some classic concepts of information retrieval and then quickly jump to the current state of the art in the field, where crawlers, spiders, and hard-of-hearing personal butlers roam."</div>
		</section>

    <input type="radio" name="accordion1" id="cb5" />
		<section class="box">
			<label class="box-title" for="cb5"><img src="images/umich.png" alt="umich" height=75px width=75px align='left' />EECS505: Computational Social Science and Machine Learning</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"Introduction to computational methods for identifying patterns and outliers in large data sets. Topics include the singular and eigenvalue decomposition, independent component analysis, graph analysis, clustering, linear, regularized, sparse and non-linear model fitting, deep, convolutional and recurrent neural networks. Students program methods; lectures and labs emphasize computational thinking and reasoning.<br>Example methods include linear and non-linear regression, neural networks, deep networks, convolutional neural networks, factor, tensorial and independent component analysis, sparsity inducing linear and non-linear regressions. Example real-world applications include foreground and background subtraction in videos, image denoising, filling in (or imputing) missing entries in an image, unmixing images and sounds, handwriting recognition, re-painting images in the style of your favorite artist and much more."</div>
		</section>

    <input type="radio" name="accordion1" id="cb6" />
		<section class="box">
			<label class="box-title" for="cb6"><img src="images/umich.png" alt="umich" height=75px width=75px align='left' />SI649: Information Visualization</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"Introduction to information visualization. Topics include data and image models, multidimensional and multivariate data, design principles for visualization, hierarchical, network, textual and collaborative visualization, the visualization pipeline, data processing for visualization, visual representations, visualization system interaction design, and impact of perception. Emphasizes construction of systems using graphics application programming interfaces (APIs) and analysis tools."</div>
		</section>

    <input type="radio" name="accordion1" id="cb7" />
		<section class="box">
			<label class="box-title" for="cb7"><img src="images/umich.png" alt="umich" height=75px width=75px align='left' />SI618: Data Manipulation and Analysis</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"This course aims to help students get started with their own data harvesting, processing, aggregation, and analysis. Data analysis is crucial to evaluating and designing solutions and applications, as well as understanding user's information needs and use. In many cases the data we need to access is distributed online among many webpages, stored in a database, or available in a large text file. Often these data (e.g. web server logs) are too large to obtain and/or process manually. Instead, we need an automated way of gathering the data, parsing it, and summarizing it, before we can do more advanced analysis. Therefore, students will learn to use Python and its modules to accomplish these tasks in a 'quick and easy' yet useful and repeatable way. Next, students will learn techniques of exploratory data analysis, using scripting, text parsing, structured query language, regular expressions, graphing, and clustering methods to explore data. R modules will be used to accomplish these tasks. Students will be able to make sense of and see patterns in otherwise intractable quantities of data."</div>
		</section>

    <input type="radio" name="accordion1" id="cb8" />
		<section class="box">
			<label class="box-title" for="cb8"><img src="images/umich.png" alt="umich" height=75px width=75px align='left' />SI507: Intermediate Programming in Python</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"The purpose of this course is to build upon the foundation provided by 506 and prepare students for more advanced courses that require programming skills. In particular, students in 507 will further develop core programming and software development skills, including: basic data structures, gathering and processing data, interactive programming, debugging the testing, using distributed code repositories, and object-oriented design. By the end of this course, students should be comfortable writing 500-1000 line object-oriented programs including data processing and interactivity, incorporate and integrate multiple third-party packages, and write effective test cases to validate a functional specification."</div>
		</section>

    <input type="radio" name="accordion1" id="cb9" />
		<section class="box">
			<label class="box-title" for="cb9"><img src="images/umich.png" alt="umich" height=75px width=75px align='left' />SI39: Design of Complex Websites</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"The purpose of this course is to provide students with all necessary skills for building and deploying web sites, as well as utilizing existing software tools. In the first half of this course we will look deeply into the ideas and concepts behind web design; particularly recent changes to HTML and CSS standards, as well as the importance of responsive web design."</div>
		</section>



		<input type="radio" name="accordion1" id="cb10" />
		<section class="box">
			<label class="box-title" for="cb10"><img src="images/usc.png" alt="usc" height=75px width=75px align='left' />ECON555: Game Theory in Economics</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"Game theory as used to understand decision making in business, economics, politics and other real-world environments. Topics covered include: basic terminology; strategic, extensive, and combinatorial models; and equilibrium strategy."</div>
		</section>

    <input type="radio" name="accordion1" id="cb11" />
		<section class="box">
			<label class="box-title" for="cb11"><img src="images/usc.png" alt="usc" height=75px width=75px align='left' />ECON510: Experimental Economics</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"Exploration of the basic theory and techniques of experimental economics. Topics include: basic game theory, experimental design, and elements of behavioral economic thought."</div>
		</section>

    <input type="radio" name="accordion1" id="cb12" />
		<section class="box">
			<label class="box-title" for="cb12"><img src="images/usc.png" alt="usc" height=75px width=75px align='left' />STAT: Elementary Statistics</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"Introduction to the fundamentals of modern statistical methods, including descriptive statistics, probability, random sampling, simple linear regression, correlation, tests of hypotheses, and estimation."</div>
		</section>

    <input type="radio" name="accordion1" id="cb13" />
		<section class="box">
			<label class="box-title" for="cb13"><img src="images/usc.png" alt="usc" height=75px width=75px align='left' />ECON508: Law and Economics</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"Economic analysis and interpretation of the law. The economic effect of current law and optimal design of law to meet social objectives."</div>
		</section>

    <input type="radio" name="accordion1" id="cb14" />
		<section class="box">
			<label class="box-title" for="cb14"><img src="images/usc.png" alt="usc" height=75px width=75px align='left' />ECON505: International Development Economics</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"Economic theories of growth in developing countries. Use of factor resources; role of social and economic institutions; use of financial trade policies for growth."</div>
		</section>

    <input type="radio" name="accordion1" id="cb15" />
		<section class="box">
			<label class="box-title" for="cb15"><img src="images/usc.png" alt="usc" height=75px width=75px align='left' />ECON436: Introductory Econometrics</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"The use of statistical techniques to analyze economic relationships. The emphasis is on the application of linear regression to real-world economic data."</div>
		</section>

		<input type="radio" name="accordion1" id="cb16" />
		<section class="box">
			<label class="box-title" for="cb16"><img src="images/ix.png" alt="usc" height=75px width=75px align='left' />iX Data Science - Cape Town</label>
			<label class="box-close" for="acc-close"></label>
			<div class="box-content">"Around the world, companies are looking for skills that many college curriculums struggle to provide. This makes it harder for students to land high quality college internships, often leaving them in a challenging position. iX bridges this gap. No matter your major or year, our programs provide you with the opportunity to learn in-demand skills – from data science to finance, consulting and more – and get industry experience."<br><br>
        Topics Covered:
        <br>Coding in Python and SQL - The industry-standard language for analyzing, building models, and presenting data.
      <br>Data Science Fundamentals - The pipeline from raw data to models in production. Version control and collaborative work.
      <br>Data Analysis - Data cleaning, manipulation, visualization, and analysis.
      <br>Statistics Foundations - Likelihood, probability distributions, hypothesis testing, and more.
      <br>Git & Version Control - Learn to collaborate with others and host your code on the internet.
      <br>Machine learning
      <br>Basics of Big Data</div>
		</section>

		<input type="radio" name="accordion1" id="acc-close" />
	</nav>
</body>

</div>







<div class='projects' id='about_me'>
<p id='project_title'>About Me<br><br></p>

<div class='about_me'>


<img src="images/me.jpeg" alt="usc" height=300px width=300 align='left'>
<p>
Hi! I'm Iain. I'm currently finishing up my second year of grad school and Michigan where I'm studying data science. Before Michigan I studied economics at the University of South Carolina. <br><br>
I'm motivated by interesting and challenging problems, and am passionate about leveraging data to help solve them. Studying economics helped me develop strong problem solving skills based on logic. It's also what motivated me to start programming, which is how I became interested in data science. Being at Michigan has given me the opportunity to grow tremendously. In my short time in Ann Arbor, I've gone from an inexperienced programmer with a light math background to someone who is confident in their ability to use programming, data, and math to solve advanced and complex problems.<br><br>
I'm an extremely hard worker, expect a lot out of myself, and give whatever I'm working on my full focus and best effort. I enjoy challenging myself and stepping outside of my comfort zone because I think it pushes me to learn and grow. I like to put myself in situations where I'm not the smartest person in the room and often times enjoy the process of failing over success. At the end of the day, I pride myself on being driven and open to learn which are things that have led me to success in the past and I think will continue to help me in the future.<br><br>
I'm a big sports fan. I love college football and basketball. Because I went to South Carolina and grew up in North Carolina I'm Gamecocks fan and follow SCar athletics pretty closely. I've played golf for close to 15 years now and love getting out to the course whenever I've got time. I really enjoy being outside and amongst nature. I have a Jeep and like to work on it whenever I have the chance. I love travelling and experiencing new cultures (I know, so cliché). I'm a big Mexican food guy, there's nothing quite like some good chips and salsa. I'm been pretty interested in following the stock market and investing. My favorite show is probably Seinfeld, I've probably watched it close to 15 times at this point. I'm trying to read more, the next book on my to-read list is Wim Hof's "The Wim Hof Method".<br><br>Thats about it I think, nothing too crazy I guess. Anyways, if you made it this far, thanks for reading! Message me if you're curious about anything or have any questions

</p>

</div>



















<div class='projects' id='contact_me'>
<p id='project_title'>Contact<br><br></p>


<div class='contact_text_box'>


  <div class='contact_text'>
  Please don't hesitate to reach out and connect with me
  </div>

  <div class='contact_icons'>
  <a id='page' href="https://www.linkedin.com/in/iain-graham-252202197/"><img src="images/linkedin.png" alt="linkedin" height=40px width=40px></a>
  <a id='page' href = "mailto: iain.martin.graham@gmail.com.com"><img src="images/mail.png" alt="mail" height=40px width=40px></a>
  <a id='page' href="https://medium.com/@imgraham1996"><img src="images/medium.png" alt="medium" height=40px width=40px></a>
  <a id='page' href="https://github.com/imgraham1"><img src="images/github-logo.png" alt="github" height=40px width=40px></a>
  </div>

  <!-- <div class='form'>



  <form
    action="https://formspree.io/f/mgeppabl"
    method="POST">
    <label>
      Your name:
      <input type="text" name="name">
    </label><br>

    <label>
      Your email:
      <input type="text" name="_replyto">
    </label><br>
    <label>
      Your message:
      <textarea name="message" rows="4" cols="50"></textarea>
    </label><br>

    <button type="submit">Send</button>
  </form>
  </div> -->




</div>
</div>


<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>





















    <footer>
      Photo by <a href="https://unsplash.com/@octoberroses?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Aubrey Odom</a> on <a href="https://unsplash.com/s/photos/grand-tetons?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>
      <br>
      <span>Photo by <a href="https://unsplash.com/@emilianobar?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Emiliano Bar</a> on <a href="https://unsplash.com/s/photos/nyc-skyline?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span><br>
      Icons made by <a href="https://www.flaticon.com/authors/dave-gandy" title="Dave Gandy">Dave Gandy</a> from <a href="https://www.flaticon.com/" title="Flaticon"> www.flaticon.com</a>
      <br>
      Icons made by <a href="https://www.flaticon.com/authors/pixel-perfect" title="Pixel perfect">Pixel perfect</a> from <a href="https://www.flaticon.com/" title="Flaticon"> www.flaticon.com</a>
      <br>
      Icons made by <a href="https://www.flaticon.com/authors/freepik" title="Freepik">Freepik</a> from <a href="https://www.flaticon.com/" title="Flaticon"> www.flaticon.com</a>
      <br>
      Image by <a href="https://pixabay.com/users/12019-12019/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1761292">David Mark</a> from <a href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1761292">Pixabay</a>
    </footer>



    <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.maxHeight){
          content.style.maxHeight = null;
        } else {
          content.style.maxHeight = content.scrollHeight + "px";
        }
      });
    }


    /* Requires jquery.js and velocity.js */
/*global jQuery:false */



    /* Javascript to close the radio button version by clicking it again */




    /* Javascript to close the radio button version by clicking it again */
    /* Javascript to close the radio button version by clicking it again */











    </script>

  </body>
</html>
